{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import XLNetTokenizer, XLNetForSequenceClassification, XLNetConfig\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Load your dataset\ndf = pd.read_csv(\"/kaggle/input/final-dataset/final_dataset\") \n\n# Split the dataset into train and test using the same random state\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Initialize XLNet tokenizer and model\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\nmodel = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=3)\n\n# Move the model to the selected device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n        # Initialize the LabelEncoder\n        self.label_encoder = LabelEncoder()\n        self.label_encoder.fit(data['Sentiment'])  # Fit the encoder on the sentiment column\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        review = self.data.iloc[idx]['reviewText']\n        sentiment = self.data.iloc[idx]['Sentiment']\n\n        encoding = self.tokenizer(review, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        # Use the LabelEncoder to convert sentiment labels to numerical labels\n        sentiment = self.label_encoder.transform([sentiment])[0]\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'labels': torch.tensor(sentiment, dtype=torch.long)  # Convert labels to torch.long\n        }\n\n# Create test dataset\nmax_len = 400\nbatch=20\ntest_dataset = CustomDataset(test_df, tokenizer, max_len)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch)\n\n# Lists to store performance metrics for both models\naccuracy_values_finetuned = []\nprecision_values_finetuned = []\nrecall_values_finetuned = []\nf1_values_finetuned = []\n\naccuracy_values_pretrained = []\nprecision_values_pretrained = []\nrecall_values_pretrained = []\nf1_values_pretrained = []\n\n# Load the model configuration\nconfig = XLNetConfig.from_pretrained(\"/kaggle/input/config-file/config.json\")\nfine_tuned_model = XLNetForSequenceClassification(config)\nfine_tuned_model.load_state_dict(torch.load(\"/kaggle/input/xlnet-ftmodel/finetuned_model.pth\"))\nfine_tuned_model = fine_tuned_model.to(device)\n\n# Ensure test dataset labels are of string data type\ntest_df['Sentiment'] = test_df['Sentiment'].astype(str)\n\n\n# Evaluate the fine-tuned model\nfine_tuned_model.eval()\npredicted_labels_finetuned = []\n\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = fine_tuned_model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        predicted_labels = torch.argmax(logits, dim=1).cpu().tolist()\n        predicted_labels_finetuned.extend(predicted_labels)\n\n    # Calculate performance metrics for the fine-tuned model\n    accuracy = accuracy_score(test_df['Sentiment'], predicted_labels_finetuned)\n    precision = precision_score(test_df['Sentiment'], predicted_labels_finetuned, average='weighted')\n    recall = recall_score(test_df['Sentiment'], predicted_labels_finetuned, average='weighted')\n    f1 = f1_score(test_df['Sentiment'], predicted_labels_finetuned, average='weighted')\n\n    accuracy_values_finetuned.append(accuracy)\n    precision_values_finetuned.append(precision)\n    recall_values_finetuned.append(recall)\n    f1_values_finetuned.append(f1)\n\n# Load the pre-trained model\npretrained_model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=3)\npretrained_model = pretrained_model.to(device)\n\n\n# Evaluate the pre-trained model\npretrained_model.eval()\npredicted_labels_pretrained = []\n\nwith tqdm(total=len(test_dataloader), desc=\"Predicting Labels for Pretrained Model\") as pbar:\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n\n            outputs = pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            predicted_labels = torch.argmax(logits, dim=1).cpu().tolist()\n            predicted_labels_pretrained.extend(predicted_labels)\n            pbar.update(1)\n    pbar.close()\n\n    # Calculate performance metrics for the pre-trained model\n    accuracy = accuracy_score(test_df['Sentiment'], predicted_labels_pretrained)\n    precision = precision_score(test_df['Sentiment'], predicted_labels_pretrained, average='weighted')\n    recall = recall_score(test_df['Sentiment'], predicted_labels_pretrained, average='weighted')\n    f1 = f1_score(test_df['Sentiment'], predicted_labels_pretrained, average='weighted')\n\n    accuracy_values_pretrained.append(accuracy)\n    precision_values_pretrained.append(precision)\n    recall_values_pretrained.append(recall)\n    f1_values_pretrained.append(f1)\n\n# Print or store the performance metrics for both models\nprint(\"Performance Metrics for Fine-Tuned Model:\")\nprint(f\"Accuracy: {accuracy_values_finetuned[0]:.4f}\")\nprint(f\"Precision: {precision_values_finetuned[0]:.4f}\")\nprint(f\"Recall: {recall_values_finetuned[0]:.4f}\")\nprint(f\"F1-Score: {f1_values_finetuned[0]:.4f}\")\n\nprint(\"Performance Metrics for Pre-Trained Model:\")\nprint(f\"Accuracy: {accuracy_values_pretrained[0]:.4f}\")\nprint(f\"Precision: {precision_values_pretrained[0]:.4f}\")\nprint(f\"Recall: {recall_values_pretrained[0]:.4f}\")\nprint(f\"F1-Score: {f1_values_pretrained[0]:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T21:06:47.668165Z","iopub.execute_input":"2023-11-05T21:06:47.668539Z","iopub.status.idle":"2023-11-05T21:11:25.199914Z","shell.execute_reply.started":"2023-11-05T21:06:47.668500Z","shell.execute_reply":"2023-11-05T21:11:25.198394Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nSome weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Calculate performance metrics for the fine-tuned model\u001b[39;00m\n\u001b[1;32m     95\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], predicted_labels_finetuned)\n\u001b[0;32m---> 96\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels_finetuned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], predicted_labels_finetuned, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m], predicted_labels_finetuned, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[1;32m   1826\u001b[0m     y_true,\n\u001b[1;32m   1827\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1834\u001b[0m ):\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \n\u001b[1;32m   1837\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1954\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1377\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:117\u001b[0m, in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMix of label input types (string and number)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n","\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"],"ename":"ValueError","evalue":"Mix of label input types (string and number)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}